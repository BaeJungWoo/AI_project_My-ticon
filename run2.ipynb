{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFqhBDkkNXGV",
        "outputId": "61a48ab1-2f8c-42bb-cbee-d9258e6dcef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: soynlp in /usr/local/lib/python3.10/dist-packages (0.0.493)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.22.4)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.4.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch_lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch_lightning) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch_lightning) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch_lightning) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.29.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji==0.6.0\n",
        "!pip install transformers\n",
        "!pip install soynlp\n",
        "!pip install pytorch_lightning\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ja2Krn-DNXGY"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import emoji\n",
        "import soynlp\n",
        "import pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mHIJx8sJNXGY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "\n",
        "from pytorch_lightning.utilities.types import EVAL_DATALOADERS\n",
        "\n",
        "class Model(LightningModule):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.training_step_outputs = []\n",
        "        self.validation_step_outputs = []\n",
        "\n",
        "        self.training_outputs = []\n",
        "        self.validation_outputs = []\n",
        "        self.training_loss = []\n",
        "        self.validation_loss = []\n",
        "\n",
        "        self.clsfier = AutoModelForSequenceClassification.from_pretrained(self.hparams.pretrained_model, num_labels=4)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.hparams.pretrained_tokenizer\n",
        "            if self.hparams.pretrained_tokenizer\n",
        "            else self.hparams.pretrained_model\n",
        "        )\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        return self.clsfier(**kwargs)\n",
        "    \n",
        "    def step(self, batch, batch_idx):\n",
        "        data, labels = batch\n",
        "        output = self(input_ids=data, labels=labels)\n",
        "\n",
        "        loss = output.loss\n",
        "        logits = output.logits\n",
        "\n",
        "        preds = logits.argmax(dim=-1)\n",
        "\n",
        "        y_true = list(labels.cpu().numpy())\n",
        "        y_pred = list(preds.cpu().numpy())\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'y_true': y_true,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \n",
        "        ret = self.step(batch, batch_idx)\n",
        "        self.training_step_outputs.append(ret)\n",
        "        return ret\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \n",
        "        ret = self.step(batch, batch_idx)\n",
        "        self.validation_step_outputs.append(ret)\n",
        "        return ret\n",
        "\n",
        "    def epoch_end(self, outputs, state='train'):\n",
        "        loss = torch.tensor(0, dtype=torch.float)\n",
        "        if state=='val': print(outputs)\n",
        "        for i in outputs:\n",
        "            loss += i['loss'].cpu().detach()\n",
        "        loss = loss / len(outputs)\n",
        "\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        for i in outputs:\n",
        "            y_true += i['y_true']\n",
        "            y_pred += i['y_pred']\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, average='macro')\n",
        "        rec = recall_score(y_true, y_pred, average='macro')\n",
        "        f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "        if state =='train':\n",
        "          self.training_outputs.append({'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1})\n",
        "          self.training_loss.append(loss)\n",
        "        elif state == 'val':\n",
        "          self.validation_outputs.append({'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1})\n",
        "          self.validation_loss.append(loss)\n",
        "\n",
        "        self.log(state+'_loss', float(loss), on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)\n",
        "        self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)\n",
        "        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}, Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')\n",
        "        return {'loss': loss}     \n",
        "\n",
        "    def on_train_epoch_end(self): # fix\n",
        "        self.epoch_end(self.training_step_outputs, state='train')\n",
        "        self.training_step_outputs.clear()\n",
        "\n",
        "    def on_validation_epoch_end(self): # fix\n",
        "        self.epoch_end(self.validation_step_outputs, state='val')   \n",
        "        self.validation_step_outputs.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        if self.hparams.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n",
        "        elif self.hparams.optimizer == 'AdamP':\n",
        "            from adamp import AdamP\n",
        "            optimizer = AdamP(self.parameters(), lr=self.hparams.lr)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n",
        "        if self.hparams.lr_scheduler == 'cos':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "        elif self.hparams.lr_scheduler == 'exp':\n",
        "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
        "        else:\n",
        "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "        }\n",
        "    \n",
        "    def read_data(self, path):\n",
        "        if path.endswith('xlsx'):\n",
        "            return pd.read_excel(path)\n",
        "        elif path.endswith('csv'):\n",
        "            return pd.read_csv(path)\n",
        "        elif path.endswith('tsv') or path.endswith('txt'):\n",
        "            return pd.read_csv(path, sep='\\t')\n",
        "        else:\n",
        "            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n",
        "        \n",
        "    def clean(self, x):\n",
        "        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
        "        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n",
        "        url_pattern = re.compile(\n",
        "            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "        x = pattern.sub(' ', x)\n",
        "        x = url_pattern.sub('', x)\n",
        "        x = x.strip()\n",
        "        x = repeat_normalize(x, num_repeats=2)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x, **kwargs):\n",
        "        return self.tokenizer.encode(\n",
        "            self.clean(str(x)),\n",
        "            padding='max_length',\n",
        "            max_length = self.hparams.max_length,\n",
        "            truncation=True,\n",
        "            **kwargs,\n",
        "        )\n",
        "    \n",
        "    def preprocess_dataframe(self, df):\n",
        "        df['sentence'] = df['sentence'].map(self.encode)\n",
        "        return df\n",
        "    \n",
        "    def dataloader(self, path, shuffle=False):\n",
        "        df = self.read_data(path)\n",
        "        df = self.preprocess_dataframe(df)\n",
        "\n",
        "        dataset = TensorDataset(\n",
        "            torch.tensor(df['sentence'].to_list(), dtype=torch.long),\n",
        "            torch.tensor(df['label'].to_list(), dtype=torch.long)\n",
        "        )\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.hparams.batch_size * 1 if not self.hparams.tpu_cores else self.hparams.tpu_cores,\n",
        "            shuffle=shuffle,\n",
        "            num_workers=self.hparams.cpu_workers\n",
        "        )\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self.dataloader(self.hparams.train_data_path, shuffle=True)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self.dataloader(self.hparams.val_data_path, shuffle=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return self.dataloader(self.hparams.test_data_path, shuffle=True)"
      ],
      "metadata": {
        "id": "ajCERsa-Rubu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def infer(model, x):\n",
        "\n",
        "    x_tokens = model.tokenizer(x, return_tensors='pt')\n",
        "    x_tokens = x_tokens.to('cuda')\n",
        "\n",
        "    return torch.softmax(\n",
        "        model(**x_tokens\n",
        "    ).logits, dim=-1)"
      ],
      "metadata": {
        "id": "uXbj4cuDR1-G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(path):\n",
        "\n",
        "    df = pd.read_csv(path, sep='\\t')\n",
        "\n",
        "    train_sentence = df['sentence']\n",
        "    train_emotion = df['emotion']\n",
        "    train_label = df['label']\n",
        "\n",
        "    return train_sentence.to_numpy(), train_emotion.to_numpy(), train_label.to_numpy()\n",
        "\n",
        "\n",
        "def idx2emoticon(path):\n",
        "    newdic = dict()\n",
        "    with open(path,'r') as f:\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if not line : break\n",
        "            newdic[int(line.split(':')[0])] = line.split(':')[1][:-1]\n",
        "    return newdic\n",
        "\n",
        "\n",
        "def emoticon2idx(path):\n",
        "    newdic = dict()\n",
        "    with open(path,'r') as f:\n",
        "        while True:\n",
        "            line = f.readline()\n",
        "            if not line : break\n",
        "            newdic[line.split(':')[0]] = line.split(':')[1][:-1]\n",
        "    return newdic\n",
        "\n",
        "\n",
        "def convert_sentence_emotion(model, emotion, sentence):\n",
        "\n",
        "    sentence_vector = []\n",
        "\n",
        "    sentence = model.encode(sentence)\n",
        "\n",
        "    for idx in range(len(sentence)):\n",
        "\n",
        "        emotion_label = emotion[idx]\n",
        "        distance = np.sqrt(np.mean(sentence[idx]**2))\n",
        "\n",
        "        tmp_vector = np.array([0, 0, 0, 0])\n",
        "        tmp_vector = np.append(tmp_vector, sentence[idx])\n",
        "        tmp_vector[emotion_label] = distance\n",
        "        sentence_vector.append(tmp_vector)\n",
        "\n",
        "    return sentence_vector\n",
        "\n",
        "\n",
        "def add_emotion(model, emotion, sentence):\n",
        "\n",
        "    sentence = model.encode(sentence)\n",
        "\n",
        "    distance = np.sqrt(np.mean(sentence**2))\n",
        "\n",
        "    emo_added = np.array([0, 0, 0, 0])\n",
        "    emo_added = np.append(emo_added, sentence)\n",
        "\n",
        "    emo_added[emotion] = distance\n",
        "\n",
        "    return emo_added"
      ],
      "metadata": {
        "id": "QJ6s2V4fR4LY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KxzXyFpsNXGZ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cmw-jzJNupB",
        "outputId": "9815ac2e-658b-4c41-f846-581715a73ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k4a3JWSBNXGZ"
      },
      "outputs": [],
      "source": [
        "emotion_model_path = '/content/drive/MyDrive/Colab Notebooks/ai project/epoch5-val_acc0.7347.ckpt'\n",
        "sentence_model_path = 'snunlp/KR-SBERT-V40K-klueNLI-augSTS'\n",
        "emoticon_sentence_path = '/content/drive/MyDrive/Colab Notebooks/ai project/myticon/dataset/sentence_emoticon.txt'\n",
        "idx2emoticon_path = '/content/drive/MyDrive/Colab Notebooks/ai project/myticon/dataset/idx2emoticon.txt'\n",
        "emoticon2idx_path = '/content/drive/MyDrive/Colab Notebooks/ai project/myticon/dataset/emoticon2idx.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wdke6YrGNXGZ"
      },
      "outputs": [],
      "source": [
        "idx2emoticon = idx2emoticon(idx2emoticon_path)\n",
        "emoticon2idx = emoticon2idx(emoticon2idx_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4wYRq_XNXGa",
        "outputId": "5f8612a4-86b5-4527-d01a-87a595988243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "emotion_model = Model.load_from_checkpoint(emotion_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JgTaSHTXNXGa"
      },
      "outputs": [],
      "source": [
        "sentence_model = SentenceTransformer(sentence_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JOSKLqPBStPV"
      },
      "outputs": [],
      "source": [
        "train_sentences, train_emotions, train_labels = load_dataset(emoticon_sentence_path)\n",
        "train_sentences = convert_sentence_emotion(sentence_model, train_emotions, train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7aDd7XiHSwi1",
        "outputId": "d74c7892-1bfd-4645-9979-791139f7e47b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='brute', metric='cosine', n_neighbors=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;, n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(algorithm=&#x27;brute&#x27;, metric=&#x27;cosine&#x27;, n_neighbors=3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3, algorithm='brute', metric='cosine')\n",
        "knn.fit(train_sentences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pAO3OO5JNXGb"
      },
      "outputs": [],
      "source": [
        "input_sentence = ['항상 밝은누나의모습이 보기좋아요']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBSjU7RmNXGb",
        "outputId": "55b52c12-3e51-46d4-a58f-622fad84c9ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sentence_emotion = infer(emotion_model, input_sentence)\n",
        "sentence_emotion\n",
        "sentence_emotion = np.argmax(sentence_emotion.cpu().detach())\n",
        "sentence_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cykw9SEUNXGb"
      },
      "outputs": [],
      "source": [
        "sentence_vector = add_emotion(sentence_model, int(sentence_emotion), input_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "78cl0VasNXGc"
      },
      "outputs": [],
      "source": [
        "emoticon_idx = knn.predict([sentence_vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q00O4c-LNXGc",
        "outputId": "b49e3603-7c36-4603-b4f5-47c197c3637b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended emoticon: ✧*.◟(ˊᗨˋ)◞.*✧\n"
          ]
        }
      ],
      "source": [
        "print('Recommended emoticon: ' + idx2emoticon[int(emoticon_idx)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#metric\n",
        "#{'슬픔':0, '기쁨':1, '분노':2, '당황':3}"
      ],
      "metadata": {
        "id": "_QI08MwnhecY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "4r-6XxBZtBK7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num = int(emoticon2idx['⸜(*ˊᗜˋ*)⸝'])"
      ],
      "metadata": {
        "id": "oqUKPHTojLq1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, train_emotions, train_labels = load_dataset(emoticon_sentence_path)"
      ],
      "metadata": {
        "id": "V-kiXw1Wimae"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.where(train_labels ==num)"
      ],
      "metadata": {
        "id": "SMGrdYwkjEUH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRnunb8HjFLZ",
        "outputId": "2184aabb-1532-43d5-d2c4-2bae2998eb80"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['두팔 벌려 볼을 밝히며 눈을 접으며 입을 벌려 웃는다', '해피데이', '너가 다시 돌아와서 기뻐!',\n",
              "       '오늘 아침 너무 상쾌하다', '웃으면 팔 벌리고 웃는다'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_sentence = list(train_sentences[idx])\n",
        "result=[]\n",
        "for i in range(5):\n",
        "  sentence_emotion = infer(emotion_model, emotion_sentence[i])\n",
        "  sentence_emotion = np.argmax(sentence_emotion.cpu().detach())\n",
        "  result.append(int(sentence_emotion))\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQWE5Qn5j3Uk",
        "outputId": "87b947be-3a2c-4cb5-b9a1-0dc0b1ef53be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 3, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freq(x):\n",
        "  cnt = Counter(x)\n",
        "  order = cnt.most_common()\n",
        "  maximum = order[0][1]\n",
        "\n",
        "  modes = []\n",
        "  for num in order:\n",
        "    if num[1] == maximum:\n",
        "      modes.append(num[0])\n",
        "  return modes"
      ],
      "metadata": {
        "id": "2RwZWFUMsLyE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcmoxDC7uGuT",
        "outputId": "47af388e-9a0b-412f-a159-3729578b120d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ai project/dataset/cus_test.csv')"
      ],
      "metadata": {
        "id": "zfDDqVPnlHRO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vk2WyYyujyx",
        "outputId": "874ba27a-d7b9-489e-b776-b02c34059474"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    3084\n",
              "2    2997\n",
              "0    2872\n",
              "1    2620\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen_list = list(test['sentence'].head(200))"
      ],
      "metadata": {
        "id": "piY5o1SkltCx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final =[]\n",
        "final2=[]\n",
        "final3=[]\n",
        "for sen in sen_list:\n",
        "  try:\n",
        "    input_emot = infer(emotion_model, sen)\n",
        "    input_emot = np.argmax(input_emot.cpu().detach())\n",
        "    sentence_vector = add_emotion(sentence_model, int(input_emot), sen)\n",
        "    emoticon_idx = knn.predict([sentence_vector])\n",
        "    pred_emot = idx2emoticon[int(emoticon_idx)]\n",
        "    pred_idx = int(emoticon2idx[pred_emot])\n",
        "    idx = np.where(train_labels ==pred_idx)\n",
        "    emotion_sentence = list(train_sentences[idx])\n",
        "    result=0\n",
        "    result_list=[]\n",
        "    for i in range(5):\n",
        "      sentence_emotion = infer(emotion_model, emotion_sentence[i])\n",
        "      sentence_emotion = np.argmax(sentence_emotion.cpu().detach())\n",
        "      result_list.append(int(sentence_emotion))\n",
        "      if int(sentence_emotion) == input_emot:\n",
        "        result = result + 1\n",
        "      \n",
        "    if input_emot in freq(result_list):\n",
        "      final2.append(1)\n",
        "    else:\n",
        "      final2.append(0)\n",
        "    \n",
        "    if input_emot in result_list:\n",
        "      final3.append(1)\n",
        "    else:\n",
        "      final3.append(0)\n",
        "    \n",
        "    final.append(result/5)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "72NNFni_lyPp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean1 = sum(final) / len(final)\n",
        "mean2  = sum(final2) / len(final2)\n",
        "mean3  = sum(final3) / len(final3)"
      ],
      "metadata": {
        "id": "lP-Xs5wxnlau"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean1,mean2,mean3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwAJuBEpn-Br",
        "outputId": "89e25951-1804-4a36-b008-9c97c4cd0f12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.47738693467336707 0.592964824120603 0.8241206030150754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UB03V0O7oCCr"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}